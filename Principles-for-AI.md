# THE ASILOMAR AI PRINCIPLES

These [principles](https://futureoflife.org/ai-principles) were developed in conjunction with the 2017 Asilomar conference ([videos here](https://futureoflife.org/bai-2017/)), through the process described [here](https://futureoflife.org/2017/01/17/principled-ai-discussion-asilomar).
 
-----------------

Artificial intelligence has already provided beneficial tools that are used every day by people around the world. Its continued development, guided by the following principles, will offer amazing opportunities to help and empower people in the decades and centuries ahead.

## Research Issues

1) Research Goal: The goal of AI research should be to create not undirected intelligence, but beneficial intelligence.

2) Research Funding: Investments in AI should be accompanied by funding for research on ensuring its beneficial use, including thorny questions in computer science, economics, law, ethics, and social studies, such as:

* How can we make future AI systems highly robust, so that they do what we want without malfunctioning or getting hacked?
* How can we grow our prosperity through automation while maintaining people’s resources and purpose?
* How can we update our legal systems to be more fair and efficient, to keep pace with AI, and to manage the risks associated with AI?
* What set of values should AI be aligned with, and what legal and ethical status should it have?

3) Science-Policy Link: There should be constructive and healthy exchange between AI researchers and policy-makers.

4) Research Culture: A culture of cooperation, trust, and transparency should be fostered among researchers and developers of AI.

5) Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards.

## Ethics and Values

6) Safety: AI systems should be safe and secure throughout their operational lifetime, and verifiably so where applicable and feasible.

7) Failure Transparency: If an AI system causes harm, it should be possible to ascertain why.

8) Judicial Transparency: Any involvement by an autonomous system in judicial decision-making should provide a satisfactory explanation auditable by a competent human authority.

9) Responsibility: Designers and builders of advanced AI systems are stakeholders in the moral implications of their use, misuse, and actions, with a responsibility and opportunity to shape those implications.

10) Value Alignment: Highly autonomous AI systems should be designed so that their goals and behaviors can be assured to align with human values throughout their operation.

11) Human Values: AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity.

12) Personal Privacy: People should have the right to access, manage and control the data they generate, given AI systems’ power to analyze and utilize that data.

13) Liberty and Privacy: The application of AI to personal data must not unreasonably curtail people’s real or perceived liberty.

14) Shared Benefit: AI technologies should benefit and empower as many people as possible.

15) Shared Prosperity: The economic prosperity created by AI should be shared broadly, to benefit all of humanity.

16) Human Control: Humans should choose how and whether to delegate decisions to AI systems, to accomplish human-chosen objectives.

17) Non-subversion: The power conferred by control of highly advanced AI systems should respect and improve, rather than subvert, the social and civic processes on which the health of society depends.

18) AI Arms Race: An arms race in lethal autonomous weapons should be avoided.

## Longer-term Issues

19) Capability Caution: There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities.

20) Importance: Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.

21) Risks: Risks posed by AI systems, especially catastrophic or existential risks, must be subject to planning and mitigation efforts commensurate with their expected impact.

22) Recursive Self-Improvement: AI systems designed to recursively self-improve or self-replicate in a manner that could lead to rapidly increasing quality or quantity must be subject to strict safety and control measures.

23) Common Good: Superintelligence should only be developed in the service of widely shared ethical ideals, and for the benefit of all humanity rather than one state or organization.


# Selected Reactions to the Principles

The following views were elicited from people interested to participate in the CL+B Fest Boston-MIT event:

---------------

It's extremely impressive and well thought out. The support is incredible and it makes sense given the forward thinking the principles are based on.

---------------

I think the principles are a great start to the project of AI ethics. They need to gain world wide acceptance and agreement. In time they can also be refined and improved upon.

---------------

I think this is very important. I have been writing about the subject since my days of doing A-life research in 1992. I tend to believe the conclusions of the book "Superintelligence", which notes that the dynamics of a situation where an AI system is designing a better version of itself are very difficult to control. An observation of the dynamics of biological evolution leads to the conclusion that speciation is often associated with "punctuation" and related extinctions (and Sapiens is the last Homo, so extinction has already been happening), although cooperative evolution is also very common.

---------------

These principles appear to be straightforward and comprehensive, this should be required reading for those entering the field. I would hope inclusive of these principles are incorporating acknowledgement of inherent bias in AI decision making along with the redeployment of human workers in the wake of AI powered automation.

---------------

I support the Asilomar AI Principles and believe in the seriousness of their adherence. When we are discussing technology that impacts the law and justice systems, it is especially important that we think carefully through the potential consequences of the systems we implement. Automated smart contracts have the potential to provide great benefit to our ability to organize as humans, but we must agree upon a common set of values in order to ensure our liberties and values are protected.

---------------

I don't disagree with anything in this document.

The legal and ethical status AI will be given should be the most interesting thing to watch unfold. 

Given the optimistic assumption that AI development will become globally collaborative, I think a major long-term goal for AI research should be creating a decentralized organization that can govern a "gated" or "self contained" AI. 

A lot of the people I know are legitimately afraid of AI. Because of this I think the public relations and media aspects of what AI can feasibly accomplish is going to be an incredibly touchy subject. 

I think that the "gated-ness" of AI should be communicated clearly to the general public in ways that the lay person can understand.

Also, there should be an equal or greater level of effort placed on the defence of a potential AI IT attack, and this effort should also be clearly communicated to the public. 

Last, AI development definitely needs to be understood by the government in a way that they can make economic adjustments pertaining to UBI and related initiatives. People need to know what new types of jobs are going to be in demand in the future.

---------------

I completely agree with the principles but I believe they will only have an effect if strict compliance from ALL nations of the world can be ensured. As such, right now, no matter how many people sign under the principles (mostly US-based) we do not know how many more will simply ignore them, or feel they are taking precautions and instead accidentally unleash the dragon. According to Ray Kurzweil singularity will occur in 2045, and since the progress is exponential, we need to be prepared for that point. Unfortunately with the current questionable political and geo-political decisions the world is becoming more fragmented. I fear that the Asilomar AI Principles will serve as "nice guidelines" for the conscious minds here in US, and the like-minded countries. But what of other places where ethics are more loosely considered, and science is mixed with government agenda, where conflict of interests may be inevitable. We can not control everything and we cannot ensure full compliance. We can only hope for human rationality and honesty. But humans by nature are irrational beings. In addition, I believe that real danger from AI will come not from labs of DeepMind or IBM Watson, but from some unknown developer that can accidentally unleash a dragon.

---------------

It's important the guideline and precaution taken by those involve with those principles.

---------------

Love it!  Helps reduce fear of the unknown.  I hope it keeps the genie in the bottle. 

---------------

My initial reaction is that it seems silly. Many of these principles are both things that we obviously want to strive for, and things that are impossible to enforce. Some of it reads as an attempt to establish a system for assigning blame when stuff breaks, which it inevitably will. Software, including AI, is never going to be perfect. Natural intelligence is never perfect either. You can't just create a bunch of rules that amount to, "People who work on AI must be held accountable if what they create isn't perfect."

We already have ethics and we already have law and we already have principles. It doesn't make sense to me that we would want to create a separate AI-specific checklist.

We need to be careful here too. We don't want to punish innovation and experimentation. Given people's paranoia about AI, I'd much rather see a list of principles that protect AI researchers from retribution if their software doesn't work the way they want it to. Of course, if you're working on a safety critical system and you make promises that you aren't able to keep, then that's an issue. Using experimental AI in such a system would be negligent.

---------------

Development of AI is a means to an end! We must remember what that end is, and come back to it over and over again when we, from time to time, forget what our intention for our work is. Also, I wonder what Elon's reaction to these principles is.

---------------

Am in total agreement, glad it exists!

---------------

I think "Asilomar Al Principles" is a good idea and I support their principles

---------------

except for the 18 - we HAVE TO defend ourselves against putLers of the world

---------------

I endorse 'Asilomar AI Principles' and believe it covers all the elements to lead us to true shared economy benefiting everyone. However, I like to add that we truly need to move away from current data exploitation by big companies and make all efforts to ensure that powerful AI technology is used for shared and prosperous economy for everyone.

---------------
long overdue.
